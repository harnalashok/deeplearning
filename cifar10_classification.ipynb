{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNmokNvR2E11abPJMcQgRGV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/deeplearning/blob/main/cifar10_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1qweqEJGod8"
      },
      "source": [
        "# 1.1 Import array-manipulation library\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# 1.2 Import tensorflow\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tbE5NQEG0Ld"
      },
      "source": [
        "\r\n",
        "# 1.3 Import keras subroutines for fetching the CIFAR-10 dataset\r\n",
        "#     The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes,\r\n",
        "#      with 6000 images per class. There are 50000 training images and 10000\r\n",
        "#       test images.  They were collected by Alex Krizhevsky, Vinod Nair, and\r\n",
        "#         Geoffrey Hinton. Classes are: airplane, automobile, bird, cat, deer,\r\n",
        "#          dog, frog, horse, ship, truck\r\n",
        "\r\n",
        "from keras.datasets import cifar10\r\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK8UHBAhIHey"
      },
      "source": [
        "# 1.4 Basic classes for specifying and training a neural network\r\n",
        "#     Keras has two types of models Sequential and Model Class for complex models\r\n",
        "#     Sequential models are essentially layer-by-layer. Model Class models may\r\n",
        "#     also have branching.\r\n",
        "\r\n",
        "from keras.models import Sequential\r\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUqAY8ASG_ce"
      },
      "source": [
        "# 1.5 Import layers that will be used in modeling\r\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Dropout, Flatten, Dense\r\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YhGwuKOI4WC"
      },
      "source": [
        "# 1.6 Keras utilities for one-hot encoding of ground truth values\r\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8lIQHroJFLK"
      },
      "source": [
        "# 1.7 Import keras optimizers unless you want default parameters\r\n",
        "# from keras.optimizers import Adam\r\n",
        "\r\n",
        "# 1.8\r\n",
        "import os, time\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "# %matplotlib inline\r\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acZbyi2iacPl"
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\r\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\r\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58Jew-jMJY2I"
      },
      "source": [
        "\r\n",
        "#%%                                 B. Define needed constants\r\n",
        "\r\n",
        "# 2.0 Set some hyperparameters\r\n",
        "\r\n",
        "# 2.1\r\n",
        "batch_size = 16   # A batch of 'batch_size' training examples is fed before \r\n",
        "                  #  every error correction  \r\n",
        "# 2.1\r\n",
        "num_epochs = 5    # Over the entire training set, we iterate 5 times \r\n",
        "                  # Thus per epoch, there will be (X_train.shape[0]/batch_size) batches\r\n",
        "# 2.3\r\n",
        "kernel_size = 3   # we will use 3x3 kernels throughout\r\n",
        "# 2.4\r\n",
        "pool_size = 2     # we will use 2x2 pooling throughout\r\n",
        "# 2.5\r\n",
        "conv_depth_1 = 32 # we will initially have 32 filters per conv. layer...\r\n",
        "                  # Remember each filter extracts some structure from image data\r\n",
        "# 2.6\r\n",
        "conv_depth_2 = 64 # ...switching to 64 filters  the first pooling layer\r\n",
        "# 2.7\r\n",
        "drop_prob_1 = 0.25 # dropout after pooling with probability 0.25\r\n",
        "# 2.8\r\n",
        "drop_prob_2 = 0.5  # dropout in the FC layer with probability 0.5\r\n",
        "# 2.9\r\n",
        "hidden_size = 512  # the FC layer will have 512 neurons\r\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "Wq7On7taJnoo",
        "outputId": "bbd488d8-e664-475b-9d77-a50dad34bc56"
      },
      "source": [
        "#%%                     C. Fetch cifar10 images & transform\r\n",
        "\"\"\"\r\n",
        "About CIFAR-10 images\r\n",
        "Ref: https://en.wikipedia.org/wiki/CIFAR-10\r\n",
        "The CIFAR-10 dataset (Canadian Institute For Advanced Research) is a\r\n",
        "collection of images that are commonly used to train machine learning\r\n",
        "and computer vision algorithms. It is one of the most widely used datasets\r\n",
        "for machine learning research. The CIFAR-10 dataset contains 60,000 32x32\r\n",
        "color images in 10 different classes. The 10 different classes represent\r\n",
        "airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.\r\n",
        "There are 6,000 images of each class. (Alex Krizhevsky)\r\n",
        "\"\"\"\r\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nAbout CIFAR-10 images\\nRef: https://en.wikipedia.org/wiki/CIFAR-10\\nThe CIFAR-10 dataset (Canadian Institute For Advanced Research) is a\\ncollection of images that are commonly used to train machine learning\\nand computer vision algorithms. It is one of the most widely used datasets\\nfor machine learning research. The CIFAR-10 dataset contains 60,000 32x32\\ncolor images in 10 different classes. The 10 different classes represent\\nairplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.\\nThere are 6,000 images of each class. (Alex Krizhevsky)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ml0KVAQJ_OV",
        "outputId": "9dea2100-8909-474e-8c93-9d20bb4682eb"
      },
      "source": [
        "#  3. Download, unzip and divide into training/test data cifar10 images\r\n",
        "#      By default download occurs at C:\\Users\\ashokharnal\\.keras\\datasets\\\r\n",
        "#      Or at /home/ashok/.keras/datasets ;  Downloaded file: cifar-10-batches-py.tar.gz.\r\n",
        "#       Expanded in folder: cifar-10-batches-py\r\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\r\n",
        "X_train.shape              # (50000, 32, 32, 3):50000 images, 32 X 32 pixels, 3-channels\r\n",
        "y_train.shape              # (50000, 1)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZRNI69dKF3t",
        "outputId": "4d72b0ea-d3e4-4f33-8ef4-5b2f5bf160af"
      },
      "source": [
        "# 3.1 Have a look at the data\r\n",
        "X_train[0,:3, :3,0]  # Pixel intensity values in the  \r\n",
        "                     #  slice of 3X3 from Ist image\r\n",
        "                     #   and Ist channel         \r\n",
        "y_train[:10,:]\r\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[59, 43, 50],\n",
              "       [16,  0, 18],\n",
              "       [25, 16, 49]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6],\n",
              "       [9],\n",
              "       [9],\n",
              "       [4],\n",
              "       [1],\n",
              "       [1],\n",
              "       [2],\n",
              "       [7],\n",
              "       [8],\n",
              "       [3]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iznx8EXRawtQ",
        "outputId": "ee756975-7d4b-4e2b-b1ca-fc165a4f4c40"
      },
      "source": [
        "# Let us check min and max values\r\n",
        "#  of pixel intensities\r\n",
        "print(X_train.min())      # 0\r\n",
        "print(X_train.max())      # 255"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zqmkT8bKSMY"
      },
      "source": [
        "\r\n",
        "# 4. There are 50000 training examples in CIFAR-10\r\n",
        "num_train, height, width, depth = X_train.shape"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VR_bj5QcKXV7",
        "outputId": "b5ba909f-b386-484a-953d-cca55d2667d4"
      },
      "source": [
        "# 4.1 There are 10000 test examples in CIFAR-10\r\n",
        "num_test = X_test.shape[0]\r\n",
        "num_test"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDg_YIhgeI3i",
        "outputId": "08fb657c-8aea-4f59-a61c-85ac6d6987b4"
      },
      "source": [
        "np.unique(y_train).shape"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aIVBTl1Ke2T",
        "outputId": "8d113d73-16c4-495e-bb1f-85b29e92174e"
      },
      "source": [
        "# 4.2 There are 10 image classes\r\n",
        "num_classes = np.unique(y_train).shape[0]\r\n",
        "num_classes"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJYt5RVWKuw2"
      },
      "source": [
        "# 4.2.1 Class names are in alphabetical sequence\r\n",
        "\r\n",
        "class_names = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\r\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nui6UWuzK6aG"
      },
      "source": [
        "\r\n",
        "# 4.2 There are 10 image classes\r\n",
        "num_classes = np.unique(y_train).shape[0]\r\n",
        "class_names = [\"airplane\",\"automobile\",\"bird\",\r\n",
        "               \"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\r\n",
        "               \"ship\",\"truck\"]\r\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot69WJIfLA7U"
      },
      "source": [
        "# 5. See an image\r\n",
        "i = 1\r\n",
        "im1 = X_train[i]    # Get the ith image array\r\n",
        "# 5.1 To which class does it belong\r\n",
        "y_train[i]\r\n",
        "k = class_names[y_train[i][0]]           # Get 0th element of y_train[1]\r\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOakNl7ULPxm",
        "outputId": "b117e843-3132-4fb0-eed8-4e338a00ffa2"
      },
      "source": [
        "print(k)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "truck\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "tOVtmh1gLiO9",
        "outputId": "05f0ad35-d89f-4865-d0a6-ad8f9b9a7d95"
      },
      "source": [
        "# 5.2 Plot the image\r\n",
        "fig = plt.figure(figsize=(4,2))\r\n",
        "plt.imshow(im1)                         # imshow() is a matplotlib method\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x144 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLQIvgWwLla_"
      },
      "source": [
        "# 5. Change array types and normalise\r\n",
        "X_train = X_train.astype('float32')\r\n",
        "X_test = X_test.astype('float32')\r\n",
        "X_train /= np.max(X_train)              # Normalise data to [0, 1] range\r\n",
        "X_test /= np.max(X_test)                # It is a global rather than column-wise (axis =0)\r\n",
        "                                        # normalization\r\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXSCUiusLqkl",
        "outputId": "822090e2-a4a2-48bc-aedc-feea97ebdfef"
      },
      "source": [
        "# 5.1 This is columnwise normalization\r\n",
        "a = np.array([[2,3,4], [50,100,200]])\r\n",
        "a = a/np.max(a, axis = 0)\r\n",
        "a\r\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.04, 0.03, 0.02],\n",
              "       [1.  , 1.  , 1.  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZItQiYWLvcM",
        "outputId": "0426756f-6fbe-4c40-fa60-c5f813bda124"
      },
      "source": [
        "\r\n",
        "# 6. One-hot encode the labels\r\n",
        "#    For all classification problems, even when num_classes is 2\r\n",
        "#    use OHE with softmax layer. See VGG16 problem for 2-classes.\r\n",
        "#    For two classes, you can also use one 'sigmoid' at the output\r\n",
        "#    without earlier performing OHE. See VGG16 problem for 2-classes.\r\n",
        "\r\n",
        "Y_train = np_utils.to_categorical(y_train, num_classes)\r\n",
        "Y_train[:5, :]\r\n",
        "y_train[:5]\r\n",
        "Y_test = np_utils.to_categorical(y_test, num_classes)\r\n",
        "Y_test[:3,:4]\r\n",
        "\r\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLhxGSNxL7fN"
      },
      "source": [
        "#%%                               D. Model building\r\n",
        "\r\n",
        "# 7. Conv [32] -> Conv [32] -> Pool (with dropout on the pooling layer)\r\n",
        "#    Same padding means the size of output feature-maps are the same as\r\n",
        "#    the input feature-maps (under the assumption of stride=1)\r\n",
        "#    padding= \"SAME\" tries to pad evenly left and right, but if the amount\r\n",
        "#    of columns to be added is odd, it will add the extra column to the right\r\n",
        "# 7.1\r\n",
        "# See keras layers: https://keras.io/layers/about-keras-layers/\r\n",
        "model = Sequential()\r\n",
        "model.add(Convolution2D(conv_depth_2,                            # 32 filters\r\n",
        "                       (kernel_size, kernel_size),               # 3 X 3\r\n",
        "                       padding='same',                           # Do zero padding\r\n",
        "                       activation='relu',\r\n",
        "                       input_shape=(height, width, depth)        # 32 X 32 X 3\r\n",
        "                       )\r\n",
        "           )\r\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gCeq0irMVfT"
      },
      "source": [
        "#7.1.1\r\n",
        "model.add(Convolution2D(conv_depth_1,\r\n",
        "                       (kernel_size, kernel_size),\r\n",
        "                        padding='valid',\r\n",
        "                        activation='relu')\r\n",
        "                        )\r\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTtGSqCdMaGo"
      },
      "source": [
        "# 7.1.2\r\n",
        "model.add(MaxPooling2D(\r\n",
        "\t                   pool_size=(pool_size, pool_size))         # 2 X 2\r\n",
        "                      )\r\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw-Rwy7WMd5O"
      },
      "source": [
        "# 7.1.3\r\n",
        "# https://keras.io/layers/core/#dropout\r\n",
        "model.add(Dropout(\r\n",
        "\t              drop_prob_1                                    # 0.25\r\n",
        "\t             )\r\n",
        "                 )\r\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yagTYsnIMj7L"
      },
      "source": [
        "# 7.2 Now flatten to 1D, apply FC -> ReLU (with dropout) -> softmax\r\n",
        "#     Fully connected layer\r\n",
        "#    https://keras.io/layers/core/#flatten\r\n",
        "model.add(Flatten())\r\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "Axat7vDKMnmT",
        "outputId": "17bbb4f7-4cca-4c3c-e176-f2ff0e7cfb85"
      },
      "source": [
        "# 7.2.1 Output of this dense layer: Ist hidden layer\r\n",
        "#       https://keras.io/layers/core/#dense\r\n",
        "\"\"\"\r\n",
        "Dense implements the operation:\r\n",
        "   output = activation(dot(input, kernel) + bias)\r\n",
        "   where activation is the element-wise activation function\r\n",
        "   passed as the activation argument, kernel is a weights matrix\r\n",
        "   created by the layer, and bias is a bias vector created by the\r\n",
        "   layer (only applicable if use_bias is True).\r\n",
        "   Size of output has to be specified\r\n",
        "\"\"\"\r\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nDense implements the operation:\\n   output = activation(dot(input, kernel) + bias)\\n   where activation is the element-wise activation function\\n   passed as the activation argument, kernel is a weights matrix\\n   created by the layer, and bias is a bias vector created by the\\n   layer (only applicable if use_bias is True).\\n   Size of output has to be specified\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV3PA7ibM04k"
      },
      "source": [
        "\r\n",
        "model.add(Dense(hidden_size, activation='relu'))   # output size = hidden_size\r\n",
        "\r\n",
        "model.add( Dropout\r\n",
        "\t             (\r\n",
        "\t             drop_prob_2                                      # 0.5\r\n",
        "\t             )\r\n",
        "\t             )\r\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "o2AfGWOaNN0_",
        "outputId": "c848e83f-bc21-4a11-d232-50eed7a85ecd"
      },
      "source": [
        "# 7.2.2 Final output layer; softmax\r\n",
        "#       About softmax: https://en.wikipedia.org/wiki/Softmax_function\r\n",
        "#       exp(xi)/Sigma(exp(xk))\r\n",
        "\"\"\"\r\n",
        "Softmax\r\n",
        "    If we take an input of [1, 2, 3, 4, 1, 2, 3], the softmax of that\r\n",
        "    is [0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]. The output\r\n",
        "    has most of its weight where the '4' was in the original input.\r\n",
        "    This is what the function is normally used for: to highlight the\r\n",
        "    largest values and suppress values which are significantly below\r\n",
        "    the maximum value.\r\n",
        "    See calculations at the end of his code.\r\n",
        "\"\"\"\r\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nSoftmax\\n    If we take an input of [1, 2, 3, 4, 1, 2, 3], the softmax of that\\n    is [0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]. The output\\n    has most of its weight where the '4' was in the original input.\\n    This is what the function is normally used for: to highlight the\\n    largest values and suppress values which are significantly below\\n    the maximum value.\\n    See calculations at the end of his code.\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFaUalsONW2o"
      },
      "source": [
        "\r\n",
        "model.add(\r\n",
        "         Dense(num_classes,\r\n",
        "         activation='softmax'\r\n",
        "         )\r\n",
        "         )\r\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWrBVbwSNaQq"
      },
      "source": [
        "# 7.3 Compile model and add necesary parameters\r\n",
        "#     Cross entropy: http://203.122.28.230/moodle/mod/url/view.php?id=1409\r\n",
        "#\r\n",
        "model.compile(loss='categorical_crossentropy',            # using the cross-entropy loss function\r\n",
        "              optimizer='adam',                           # using the Adam optimiser\r\n",
        "                                                          # Ref: https://keras.io/optimizers/\r\n",
        "                                                          # keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\r\n",
        "              metrics=['accuracy'])                       # reporting the accuracy\r\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56xxdp9sNfX8",
        "outputId": "656f39da-7e85-47df-e09b-e3d647aca4aa"
      },
      "source": [
        "\r\n",
        "#%%                              E. Model training and evaluation\r\n",
        "# 8. ...holding out 10% of the data for validation. 26 minutes\r\n",
        "#       To save time consider just 20000 samples for training\r\n",
        "#       10% of these, ie 2000 are for validation\r\n",
        "\r\n",
        "# 8.1\r\n",
        "X_train.shape\r\n",
        "Y_train.shape\r\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2Foh8ZtNloJ",
        "outputId": "2f1862b0-8154-4936-ec0a-44c25a34c9b6"
      },
      "source": [
        "# 8.2  Takes 20 minutes\r\n",
        "#      Specify fit/train hyperparameters\r\n",
        "start = time.time()\r\n",
        "history = model.fit(X_train[:20000],   # Train model using limited training data\r\n",
        "                    Y_train[:20000],\r\n",
        "                    batch_size=batch_size,\r\n",
        "                    epochs=num_epochs,\r\n",
        "                    verbose=1,\r\n",
        "                    validation_split=0.1\r\n",
        "                    )\r\n",
        "end = time.time()\r\n",
        "print ((end - start)/60)\r\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1125/1125 [==============================] - 103s 91ms/step - loss: 1.8628 - accuracy: 0.3153 - val_loss: 1.3667 - val_accuracy: 0.5135\n",
            "Epoch 2/5\n",
            "1125/1125 [==============================] - 103s 92ms/step - loss: 1.3174 - accuracy: 0.5307 - val_loss: 1.2278 - val_accuracy: 0.5660\n",
            "Epoch 3/5\n",
            "1125/1125 [==============================] - 104s 92ms/step - loss: 1.1150 - accuracy: 0.6032 - val_loss: 1.1834 - val_accuracy: 0.5935\n",
            "Epoch 4/5\n",
            "1125/1125 [==============================] - 103s 92ms/step - loss: 0.9846 - accuracy: 0.6504 - val_loss: 1.1210 - val_accuracy: 0.6060\n",
            "Epoch 5/5\n",
            "1125/1125 [==============================] - 103s 92ms/step - loss: 0.8466 - accuracy: 0.7050 - val_loss: 1.1508 - val_accuracy: 0.6130\n",
            "8.616279705365498\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fryhojjmUfLt"
      },
      "source": [
        "\r\n",
        "# 8.3 Certain parameters are associated\r\n",
        "#     with 'history' object\r\n",
        "history.epoch\r\n",
        "history.params\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3RbAYyGU7DN"
      },
      "source": [
        "\r\n",
        "# 8.4 How accuracy changes as epochs increase\r\n",
        "#     We will use this function agai and again\r\n",
        "#     in subsequent examples\r\n",
        "\r\n",
        "def plot_history():\r\n",
        "    val_acc = history.history['val_acc']\r\n",
        "    tr_acc=history.history['acc']\r\n",
        "    epochs = range(1, len(val_acc) +1)\r\n",
        "    plt.plot(epochs,val_acc, 'b', label = \"Validation accu\")\r\n",
        "    plt.plot(epochs, tr_acc, 'r', label = \"Training accu\")\r\n",
        "    plt.title(\"Training and validation accuracy\")\r\n",
        "    plt.legend()\r\n",
        "    plt.show()\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}